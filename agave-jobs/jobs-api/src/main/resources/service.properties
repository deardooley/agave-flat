################################################################################
################################################################################
#
# 				Agave Job Service Configuration File
#
################################################################################
################################################################################

################################################################################
# Service debug parameters
################################################################################

iplant.debug.mode=false
iplant.debug.username=
iplant.api.version=${foundation.api.version}
iplant.service.version=${foundation.service.version}

################################################################################
# Service response parameters
################################################################################

iplant.default.page.size=${foundation.api.page.size.default}
iplant.max.page.size=${foundation.api.page.size.max}

################################################################################
# Service authentication parameters
################################################################################

# Specifies the type of authentication to use. Should be one of
# none, ldap, ldap+tacc, api. Currently, api is sufficient to support
# iPlant community usage only.
iplant.auth.source=${foundation.auth.source}
iplant.community.username=${iplant.community.username}
iplant.community.password=${iplant.community.password}

iplant.ldap.url=${foundation.ldap.url}
iplant.ldap.base.dn=${foundation.ldap.dn}

# comma separated list of trusted users. the self-named .crt files must be in the classes/certs folder
iplant.trusted.users=iplant-dev,ipcservices,dooley,vaughn,lenards,denni,wregglej,healyk,psarando,hariolf,sriram

iplant.myproxy.server=${iplant.community.myproxy.server}
iplant.myproxy.port=${iplant.community.myproxy.port}

################################################################################
# Service mail settings
################################################################################

mail.smtps.provider=${foundation.smtp.provider}
mail.smtps.auth=${foundation.smtp.auth}
mail.smtps.user=${foundation.smtp.user}
mail.smtps.passwd=${foundation.smtp.password}
mail.smtps.host=${foundation.smtp.host}
mail.smtps.port=${foundation.smtp.port}
mail.smtps.from.name=${foundation.smtp.from.name}
mail.smtps.from.address=${foundation.smtp.from.address}

################################################################################
# Service scaling parameters
################################################################################

# This parameter tells the service whether to start in master or slave mode. Master
# mode is the full service with all endpoints enabled. Slave mode starts the queues,
# but disables the service from accepting new jobs. It is primarily used to run worker
# instances to process jobs from  the main service. true for master, false otherwise
iplant.slave.mode=${foundation.service.slave.mode}

# This parameter tells the service whether it is running as a condor node. If so,
# it will only process condor jobs. The archive queue will only archvie condor jobs.
# This is needed because all data for condor jobs is cached on the condor nodes
# and no other service will be able to access it.
iplant.condor.gateway.node=${foundation.service.jobs.gateway.node}

# This parameter tells the service, when running as a condor gateway, what system
# to process jobs from. Condor is difficult to monitor remotely, so all condor
# jobs must be monitored with an execute worker and monitor process on the
# server where jobs are submitted from. If this is not set, no monitoring
# processes will be run, so it will be up to another process to determine when
# jobs have finished and alert the API via a trigger callback.
iplant.local.system.id=${foundation.service.jobs.local.system.id}

# whether to allow fork jobs to run on this instance. If running a single instance
# of this service, think through this decision carefully as the child processes
# are synchronous, blocking calls that could eat up quite a bit of memory and
# potentially bring down the server.
iplant.allow.fork=${foundation.service.jobs.allow.fork}

###################################################
#           SCHEDULER and WORKER Parameters
###################################################
# Use the following switch to configure the queue-based schedulers and workers.
# The default values are actually defined in jobs-core's Settings.java file.

# Set to true to start a jobs service instance that will run schedulers.  Each job processing
# phase has its own scheduler: STAGING, SUBMITTING, MONITORING, ARCHIVING.  Multiple instances
# of the job service can run schedulers, though only one scheduler for each phase runs at a time.
# The service must start in one of 3 modes:  scheduler, worker, or scheduler and worker.
#
# Default = true
iplant.service.jobs.scheduler.mode=${foundation.service.jobs.scheduler.mode}

# Set to true to start a jobs service instance that will run workers.  Each queue has its
# own set of workers as defined in the tenant's queue definition resource (see jobs-core
# resources/queueConfigurations).  The service must start in one of 3 modes:  scheduler, worker, 
# or scheduler and worker.
#
# Default = true 
iplant.service.jobs.worker.mode=${foundation.service.jobs.worker.mode}

# Set to true to enable a background thread to run and clean up any jobs found in a zombie state. 
# Zombie jobs are those that are not making sufficient progress so they are automatically rolled back.
#
# Default = false
iplant.service.jobs.enable.zombie.cleanup=${foundation.service.jobs.enable.zombie.cleanup}

# The maximum number of times to check if a worker thread has released its claim on a job during
# rollback processing.  The thread coordinating the rollback will wait claim.poll.sleep.ms 
# milliseconds between each check.
#
# Default = 15
iplant.service.jobs.claim.poll.iterations=${foundation.service.jobs.claim.poll.iterations}

# The number of milliseconds to wait between checks during rollback processing.  Each check
# queries the database to determine if a specific job has been released.
#
# Default = 1,000 milliseconds
iplant.service.jobs.claim.poll.sleep.ms=${foundation.service.jobs.claim.poll.sleep.ms}

# The maximum number of job uuids that a scheduler can specify in a single query.  This
# number bounds the size of a query that retrieves jobs by uuid and, in effect, limits
# the number of full job records that can be returned at one time from the database to a 
# scheduler.
#
# Default = 1,000
iplant.service.jobs.uuid.query.limit=${foundation.service.jobs.uuid.query.limit}

# The number of seconds a scheduler lease lasts.  Only one scheduler for each phase obtains the 
# lease that allows it to schedule jobs.  The scheduler must renew its lease before the lease
# expires.  When a lease expires, any executing scheduler may acquire the lease for a phase,
# locking out all other schedulers for that phase.  
#
# A rule of thumb is for the lease to last at least as long as 10 times the scheduler.normal.poll.ms 
# PLUS the largest expected run time for processing a batch of jobs.  Individual workloads may require 
# fine tuning these parameters to avoid leases expiring because a scheduler is too busy to renew its 
# lease on time.  Using the default value for this parameter and scheduler.normal.poll.ms, the 
# ratio of least to polling delay is 1:25.  
#
# Default = 250 seconds (4 minutes, 10 seconds)
iplant.service.jobs.scheduler.lease.seconds=${foundation.service.jobs.scheduler.lease.seconds}

# The number of milliseconds that a scheduler waits before querying the database for new work.
# The wait period between polling calls should be sufficiently small so that schedulers can do
# their work and still have time to renew their leases.  See the above discussion on 
# scheduler.lease.seconds for details.
#
# The actual delay may be up to a second longer than the configured value.  A random number of
# milliseconds between 0 and 999 is added to the configured value help stagger the polling
# done across all schedulers. 
#
# Default = 10,000 milliseconds 
iplant.service.jobs.scheduler.normal.poll.ms=${foundation.service.jobs.scheduler.normal.poll.ms}

# The number of seconds that an interrupt exist before being a candidate for clean up.  An interrupt
# sent to a specific job may or may not ever be seen by a worker thread.  An interrupt is considered
# stale after the number of seconds specified here passes and will be cleaned up at first opportunity.
#
# Default = 3,600 seconds (1 hour) 
iplant.service.jobs.interrupt.ttl.seconds=${foundation.service.jobs.interrupt.ttl.seconds}

# The number of milliseconds a reaper thread waits between interrupt clean up cycles.  The reaper
# sleeps for this specified number of milliseconds and then queries the interrupts table for 
# stale interrupts.  Stale interrupts are then deleted.
#
# Default = 240,00 milliseconds (4 minutes)
iplant.service.jobs.interrupt.delete.ms=${foundation.service.jobs.interrupt.delete.ms}

# The number of milliseconds that the zombie clean up thread waits between checks.  Zombie jobs
# are those that are not making sufficient progress so they are automatically rolled back.
#
# Default = 600,000 milliseconds (10 minutes)
iplant.service.jobs.zombie.monitor.ms=${foundation.service.jobs.zombie.monitor.ms}

# The number of milliseconds to wait for all other threads in the same jobs instance to 
# terminate after an interrupt is sent to all threads in the service.  This is part of the
# shutdown and interrupt handling mechanisms that works in conjunction with thread.death.poll.ms.
#
# Default = 10,000 milliseconds (10 seconds)
iplant.service.jobs.thread.death.ms=${foundation.service.jobs.thread.death.ms}

# The number of milliseconds to sleep before determining if all threads have terminated after
# being interrupted.  This works in conjunction with thread.death.ms.
#
# Default = 100 milliseconds
iplant.service.jobs.thread.death.poll.ms=${foundation.service.jobs.thread.death.poll.ms}

# The number of milliseconds to wait when closing connection to the queue broker before
# recording an error and continuing.
#
# Default = 5000 milliseconds
iplant.service.jobs.connection.close.ms=${foundation.service.jobs.connection.close.ms}

# The minimum number of milliseconds between attempts that a worker thread makes to initialize 
# to its queue and read from it.  The time specified here is added to a random number of 
# seconds between 0 and 10 to determine exactly how long the worker waits before retrying.
# The randomized approach is used to prevent stagger network requests in cases where many
# threads experience an error at the same time (e.g., a network failure). 
#
# Default = 10,000 milliseconds  
iplant.service.jobs.worker.init.retry.ms=${foundation.service.jobs.worker.init.retry.ms}

# The maximum number of times to retry a job submission.
#
# Default = 0
iplant.service.jobs.max.submission.retries=${foundation.service.jobs.max.submission.retries}

# The subfolder of resources/queueConfigurations from which we read all queue definitions.
# The definitions are in json file, one for each tenant id.  See TenantQueues.updateAll() for 
# details.
#
# Default = "basic"
iplant.service.jobs.queue.config.folder=${foundation.service.jobs.queue.config.folder}

################################################################################
# Service job submission parameters
################################################################################

#commands disabled in the submit scripts. They will be replace with an error message
iplant.blacklist.commands=iadmin,icp,iexit,iinit,imeta,iphybun,iqdel,ireg,irule,ixmsg,ibun,idbug,ifsck,ilocate,imiscsvrinfo	iphymv,iqmod,irepl,iscan,icd,ienv,ils,imkdir,ips,iqstat,irm,isysmeta,ichksum,ierror,igetwild.sh,ilsresc,imv,iput,iquest,irmtrash,itrim,ichmod,iexecmd,ihelp,imcoll,ipasswd,ipwd,iquota,irsync,iuserinfo,passwd,sudo

# files/folders ignored during app staging to the host system
iplant.blacklist.files=.csv,.irods,.git,.svn,.mvn,.ssh

# location of java keystore containing the cert for the ldap server above
system.keystore.path=${foundation.keystore.path}
system.truststore.path=${foundation.trusted.certs.path}
system.ca.certs.path=${foundation.ca.certs.path}

# set to the textual id of a tenant to enable work for just this tenant
# this will not preclude other tenants from accepting work for this
# tenant
iplant.dedicated.tenant.id=${foundation.service.dedicated.tenant}

# set to a comma-separated list of systems who will receive exclusive
# use of this worker for processing their activity. To isolate further
# by queue, specify the systems as <system_id>#<queue_name>
iplant.dedicated.system.id=${foundation.service.dedicated.systems}

# set to a comma-separated list of users who will receive exclusive
# use of this worker for processing their activity
iplant.dedicated.user.id=${foundation.service.dedicated.users}

# set to true to prevent the queues from accepting any more work.
# this is checked in real time.
iplant.drain.all.queues=${foundation.service.drain.all.queues}

###################################################
# 				MESSAGING SERVICE
###################################################

# specify the messaging service you want to use to handle messaging
# across the api. Valid values are rabbitmq, ironmq, and beanstalk
iplant.messaging.provider=${foundation.service.messaging.provider}

iplant.messaging.username=${foundation.service.messaging.username}
iplant.messaging.password=${foundation.service.messaging.password}
iplant.messaging.host=${foundation.service.messaging.host}
iplant.messaging.port=${foundation.service.messaging.port}

###################################################
# 			MESSAGING QUEUES & TOPICS
###################################################

# This is the queue that the notification workers will listen on
# to process registered webhooks and email notifications.
iplant.notification.service.queue=${foundation.service.notif.queue}
iplant.notification.service.topic=${foundation.service.notif.topic}

iplant.max.notification.retries=${foundation.service.notif.max.retries}

###################################################
# 				Dependent Services
###################################################
#dev
#iplant.internal.account.service.secret=3y902q9puzgkmw999sv1ph
#iplant.internal.account.service.key=a870a2b0047ca50754ce1f0c4662e7469ed2dd49
iplant.service.documentation=${foundation.service.documentation}
iplant.internal.account.service=${foundation.service.profiles.trellis.url}
iplant.internal.account.service.secret=${foundation.service.profiles.trellis.secret}
iplant.internal.account.service.key=${foundation.service.profiles.trellis.key}
iplant.atmosphere.service=${foundation.service.atmosphere}
iplant.app.service=${foundation.service.apps}
iplant.auth.service=${foundation.service.auth}
iplant.io.service=${foundation.service.files}
iplant.job.service=${foundation.service.jobs}
iplant.log.service=${foundation.service.log}
iplant.metadata.service=${foundation.service.meta}
iplant.monitor.service=${foundation.service.monitor}
iplant.notification.service=${foundation.service.notif}
iplant.postit.service=${foundation.service.postits}
iplant.profile.service=${foundation.service.profiles}
iplant.system.service=${foundation.service.systems}
iplant.transfer.service=${foundation.service.transfers}
iplant.transform.service=${foundation.service.transforms}

################################################################################
# IRODS access info.
#
# The username and password are kept here. The access
# info is served from a separate web server so changes can be made to multiple
# instances instantly without downtime.
################################################################################

iplant.irods.username=${irods.username}
iplant.irods.password=${irods.password}
iplant.irods.host=${irods.host}
iplant.irods.port=${irods.port}
iplant.irods.zone=${irods.zone}
iplant.irods.default.resource=${irods.resource}

# user account for wide open public access
iplant.world.user=${foundation.world.username}

# user account for all authenticated users
iplant.public.user=${foundation.public.username}
