################################################################################
################################################################################
#
# 				Agave Job Service Configuration File
#
################################################################################
################################################################################

################################################################################
# Service debug parameters
################################################################################

iplant.debug.mode=false
iplant.debug.username=
iplant.api.version=${foundation.api.version}
iplant.service.version=${foundation.service.version}

################################################################################
# Service response parameters
################################################################################

iplant.default.page.size=${foundation.api.page.size.default}
iplant.max.page.size=${foundation.api.page.size.max}

################################################################################
# Service authentication parameters
################################################################################

# Specifies the type of authentication to use. Should be one of
# none, ldap, ldap+tacc, api. Currently, api is sufficient to support
# iPlant community usage only.
iplant.auth.source=${foundation.auth.source}
iplant.community.username=${iplant.community.username}
iplant.community.password=${iplant.community.password}

iplant.ldap.url=${foundation.ldap.url}
iplant.ldap.base.dn=${foundation.ldap.dn}

# comma separated list of trusted users. the self-named .crt files must be in the classes/certs folder
iplant.trusted.users=iplant-dev,ipcservices,dooley,vaughn,lenards,denni,wregglej,healyk,psarando,hariolf,sriram

iplant.myproxy.server=${iplant.community.myproxy.server}
iplant.myproxy.port=${iplant.community.myproxy.port}


################################################################################
# Service mail settings
################################################################################

mail.smtps.provider=${foundation.smtp.provider}
mail.smtps.auth=${foundation.smtp.auth}
mail.smtps.user=${foundation.smtp.user}
mail.smtps.passwd=${foundation.smtp.password}
mail.smtps.host=${foundation.smtp.host}
mail.smtps.port=${foundation.smtp.port}
mail.smtps.from.name=${foundation.smtp.from.name}
mail.smtps.from.address=${foundation.smtp.from.address}

################################################################################
# Service scaling parameters
################################################################################

# This parameter tells the service whether to start in master or slave mode. Master
# mode is the full service with all endpoints enabled. Slave mode starts the queues,
# but disables the service from accepting new jobs. It is primarily used to run worker
# instances to process jobs from  the main service. true for master, false otherwise
iplant.slave.mode=${foundation.service.slave.mode}

# This parameter tells the service whether it is running as a condor node. If so,
# it will only process condor jobs. The archive queue will only archvie condor jobs.
# This is needed because all data for condor jobs is cached on the condor nodes
# and no other service will be able to access it.
iplant.condor.gateway.node=${foundation.service.jobs.gateway.node}

# This parameter tells the service, when running as a condor gateway, what system
# to process jobs from. Condor is difficult to monitor remotely, so all condor
# jobs must be monitored with an execute worker and monitor process on the
# server where jobs are submitted from. If this is not set, no monitoring
# processes will be run, so it will be up to another process to determine when
# jobs have finished and alert the API via a trigger callback.
iplant.local.system.id=${foundation.service.jobs.local.system.id}

# whether to allow fork jobs to run on this instance. If running a single instance
# of this service, think through this decision carefully as the child processes
# are synchronous, blocking calls that could eat up quite a bit of memory and
# potentially bring down the server.
iplant.allow.fork=${foundation.service.jobs.allow.fork}

################################################################################
# Service job submission parameters
################################################################################

# maximum number of worker tasks running to submit queued job requests
iplant.max.submission.tasks=${foundation.service.jobs.submission.tasks}

# maximum number of staging tasks running to initiate file staging into irods via the IO service.
# generally speaking, this should be less than the number of worker tasks since it's doing async
# calls to the IO service.
iplant.max.staging.tasks=${foundation.service.jobs.staging.tasks}

# maximum number of archive tasks running to move output job data from the compute resources
# back to irods after jobs complete or fail. If set to 0, no data will be archived.
iplant.max.archive.tasks=${foundation.service.jobs.archiving.tasks}

# maximum number of monitoring tasks that will be started up to watch job statuses.
iplant.max.monitoring.tasks=${foundation.service.jobs.monitoring.tasks}

# maximum number of times the service will retry putting a job into queue
iplant.max.submission.retries=${foundation.service.jobs.max.retries}

# maximum simultaneous queued + running jobs on any given system for a user.
#iplant.max.user.jobs.per.system=10

# maximum concurrent transfers per user. -1 or blank for no limit
iplant.max.user.concurrent.transfers=${foundation.service.transfers.max.simultaneous}

# root work directory on hpc systems
#iplant.work.path=${foundation.local.scratch.dir}

# local temp directory on server for staging execution folders.
iplant.server.temp.dir=${foundation.local.scratch.dir}

# account against which to charge the sus
iplant.charge.number=${iplant.community.charge.number}

#commands disabled in the submit scripts. They will be replace with an error message
iplant.blacklist.commands=iadmin,icp,iexit,iinit,imeta,iphybun,iqdel,ireg,irule,ixmsg,ibun,idbug,ifsck,ilocate,imiscsvrinfo	iphymv,iqmod,irepl,iscan,icd,ienv,ils,imkdir,ips,iqstat,irm,isysmeta,ichksum,ierror,igetwild.sh,ilsresc,imv,iput,iquest,irmtrash,itrim,ichmod,iexecmd,ihelp,imcoll,ipasswd,ipwd,iquota,irsync,iuserinfo,passwd,sudo

# files/folders ignored during app staging to the host system
iplant.blacklist.files=.csv,.irods,.git,.svn,.mvn,.ssh

# location of java keystore containing the cert for the ldap server above
system.keystore.path=${foundation.keystore.path}
system.truststore.path=${foundation.trusted.certs.path}
system.ca.certs.path=${foundation.ca.certs.path}

# set to the textual id of a tenant to enable work for just this tenant
# this will not preclude other tenants from accepting work for this
# tenant
iplant.dedicated.tenant.id=${foundation.service.dedicated.tenant}

# set to a comma-separated list of systems who will receive exclusive
# use of this worker for processing their activity. To isolate further
# by queue, specify the systems as <system_id>#<queue_name>
iplant.dedicated.system.id=${foundation.service.dedicated.systems}

# set to a comma-separated list of users who will receive exclusive
# use of this worker for processing their activity
iplant.dedicated.user.id=${foundation.service.dedicated.users}

# set to true to prevent the queues from accepting any more work.
# this is checked in real time.
iplant.drain.all.queues=${foundation.service.drain.all.queues}

################################################################################
# Trigger service settings
################################################################################



################################################################################
# Service policy parameters
################################################################################

# set to -1 to disable disk quotas
iplant.user.disk.quota=${foundation.service.files.disk.quota}

# how many transform threads to start up
iplant.max.transform.tasks=${foundation.service.transforms.max.tasks}

# how many times to retry failed staging transfers
iplant.max.staging.retries=${foundation.service.transfers.max.retries}

# if true, proxy transfers will be downloaded and uploaded to disk using the
# native transfer utilities rather than copied in memory. This eats up a lot
# of disk, but can be much better performing.
iplant.allow.relay.transfers=${foundation.allow.relay.transfers}

# maximum file size that can be relayed in GB
iplant.max.relay.transfer.size=${foundation.max.relay.transfer.size}

# if true, a background process will run in the background to clean up any
# jobs found in a zombie state across the platform.
iplant.enable.zombie.cleanup=false

###################################################
# 				MESSAGING SERVICE
###################################################

# specify the messaging service you want to use to handle messaging
# across the api. Valid values are rabbitmq, ironmq, and beanstalk
iplant.messaging.provider=${foundation.service.messaging.provider}

iplant.messaging.username=${foundation.service.messaging.username}
iplant.messaging.password=${foundation.service.messaging.password}
iplant.messaging.host=${foundation.service.messaging.host}
iplant.messaging.port=${foundation.service.messaging.port}

###################################################
# 			MESSAGING QUEUES & TOPICS
###################################################

# This is the queue that the notification workers will listen on
# to process registered webhooks and email notifications.
iplant.notification.service.queue=${foundation.service.notif.queue}
iplant.notification.service.topic=${foundation.service.notif.topic}

iplant.max.notification.retries=${foundation.service.notif.max.retries}

###################################################
# 				Dependent Services
###################################################
#dev
#iplant.internal.account.service.secret=3y902q9puzgkmw999sv1ph
#iplant.internal.account.service.key=a870a2b0047ca50754ce1f0c4662e7469ed2dd49
iplant.service.documentation=${foundation.service.documentation}
iplant.internal.account.service=${foundation.service.profiles.trellis.url}
iplant.internal.account.service.secret=${foundation.service.profiles.trellis.secret}
iplant.internal.account.service.key=${foundation.service.profiles.trellis.key}
iplant.atmosphere.service=${foundation.service.atmosphere}
iplant.app.service=${foundation.service.apps}
iplant.auth.service=${foundation.service.auth}
iplant.io.service=${foundation.service.files}
iplant.job.service=${foundation.service.jobs}
iplant.log.service=${foundation.service.log}
iplant.metadata.service=${foundation.service.meta}
iplant.monitor.service=${foundation.service.monitor}
iplant.notification.service=${foundation.service.notif}
iplant.postit.service=${foundation.service.postits}
iplant.profile.service=${foundation.service.profiles}
iplant.system.service=${foundation.service.systems}
iplant.transfer.service=${foundation.service.transfers}
iplant.transform.service=${foundation.service.transforms}

################################################################################
# IRODS access info.
#
# The username and password are kept here. The access
# info is served from a separate web server so changes can be made to multiple
# instances instantly without downtime.
################################################################################

iplant.irods.username=${irods.username}
iplant.irods.password=${irods.password}
iplant.irods.host=${irods.host}
iplant.irods.port=${irods.port}
iplant.irods.zone=${irods.zone}
iplant.irods.default.resource=${irods.resource}

# user account for wide open public access
iplant.world.user=${foundation.world.username}

# user account for all authenticated users
iplant.public.user=${foundation.public.username}
